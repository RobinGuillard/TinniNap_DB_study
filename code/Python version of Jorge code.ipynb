{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:/Documents/Thèse EDISCE/TinniNap_DB_study/2021-sleeptin-master/data/\")\n",
    "sleep_data = pd.read_csv(\"sleep_data_271022.csv\", na_values=[-1, \"\", \"NA\", \"N/A\", \"0000-00-00\"],\n",
    "    # Tell pandas to use all rows to guess the column types\n",
    "    low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sleep_data.columns = sleep_data.columns.str.replace(\"question_\", \"\")\n",
    "sleep_data.columns = sleep_data.columns.str.replace(\"tinnitus_questionnaire_gundh_\", \"tq_\")\n",
    "sleep_data.columns = sleep_data.columns.str.replace(\"exp_\", \"\")\n",
    "sleep_data.columns = sleep_data.columns.str.replace(\"whoqol_bref_domain\", \"whoqol_\")\n",
    "sleep_data.columns = sleep_data.columns.str.replace(\"tinnitus_severity_\", \"ts_\")\n",
    "sleep_data.columns = sleep_data.columns.str.replace(\"_q_\", \"_\")\n",
    "sleep_data.columns = sleep_data.columns.str.replace(\"v_\", \"\")\n",
    "sleep_data.columns = sleep_data.columns.str.replace(\"audiological_examination_\", \"audio_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-149-fa58beafe0f1>:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"thi_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"thi_\")]].replace(-1, pd.np.nan)\n",
      "<ipython-input-149-fa58beafe0f1>:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"tq_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"tq_\")]].replace(-1, pd.np.nan)\n",
      "<ipython-input-149-fa58beafe0f1>:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"tfi_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"tfi_\")]].replace(-1, pd.np.nan)\n",
      "<ipython-input-149-fa58beafe0f1>:5: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"ts_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"ts_\")]].replace(-1, pd.np.nan)\n",
      "<ipython-input-149-fa58beafe0f1>:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"whoqol_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"whoqol_\")]].replace(-1, pd.np.nan)\n"
     ]
    }
   ],
   "source": [
    "# replace -1 with NaN\n",
    "sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"thi_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"thi_\")]].replace(-1, pd.np.nan)\n",
    "sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"tq_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"tq_\")]].replace(-1, pd.np.nan)\n",
    "sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"tfi_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"tfi_\")]].replace(-1, pd.np.nan)\n",
    "sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"ts_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"ts_\")]].replace(-1, pd.np.nan)\n",
    "sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"whoqol_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.startswith(\"whoqol_\")]].replace(-1, pd.np.nan)\n",
    "\n",
    "# set the selected columns to numeric\n",
    "sleep_data[sleep_data.columns[sleep_data.columns.str.contains(\"_question_\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.contains(\"_question_\")]].apply(pd.to_numeric, errors='coerce')\n",
    "sleep_data[sleep_data.columns[sleep_data.columns.str.endswith(\"_score\")]] = sleep_data[sleep_data.columns[sleep_data.columns.str.endswith(\"_score\")]].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data['tschq_q7_begin_correlation']= sleep_data['tschq_q7_begin_correlation'].str.slice(0,1)\n",
    "sleep_data = sleep_data.assign(\n",
    "    # Convert numeric columns that start with \"tschq_\" to categorical\n",
    "    **{col: pd.Categorical(sleep_data[col]) for col in sleep_data.columns[(sleep_data.dtypes == \"float64\") & sleep_data.columns.str.startswith(\"tschq_\")]},\n",
    "    sex=pd.Categorical(sleep_data['sex'].replace({'male': '0', 'female': '1'})))\n",
    "\n",
    "sleep_data['tschq_q7_begin_correlation'] = sleep_data['tschq_q7_begin_correlation'].replace({'x': np.nan, ',': np.nan})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '5', '0', '1', nan, '2', '3']\n",
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#colonne maudite ! 'tschq_q7_begin_correlation'\n",
    "#we should only have numeric encodings :\n",
    "print(list(sleep_data['tschq_q7_begin_correlation'].unique()))\n",
    "#print(list(sleep_data['tschq_months_since_begin_tinnitus'].unique()))\n",
    "print(list(sleep_data['sex'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep_data['tschq-personal_loudness'] = sleep_data['tschq_q12_personal_volume']\n",
      "sleep_data['tschq-months_onset'] = sleep_data['tschq_months_since_begin_tinnitus']\n",
      "sleep_data['tschq-awareness_time'] = sleep_data['tschq_q16_awareness']\n",
      "sleep_data['tschq-angerness_time'] = sleep_data['tschq_q17_angerness']\n",
      "sleep_data['tschq-psychiatry'] = sleep_data['tschq_q35_psychological_treatment']\n",
      "sleep_data['tschq-neckpain'] = sleep_data['tschq_q33_nape_problems']\n",
      "sleep_data['tschq-tmj'] = sleep_data['tschq_q32_jaw_problems']\n",
      "sleep_data['tschq-vertigo'] = sleep_data['tschq_q31_bogus']\n",
      "sleep_data['tschq-headache'] = sleep_data['tschq_q30_headache']\n",
      "sleep_data['tschq-noise-pain'] = sleep_data['tschq_q29_noise_dependent']\n",
      "sleep_data['tschq-hyperacusis'] = sleep_data['tschq_q28_noise_sensitive']\n",
      "sleep_data['tschq-ha'] = sleep_data['tschq_q27_hear_helps']\n",
      "sleep_data['tschq-hearproblems'] = sleep_data['tschq_q26_hear_problems']\n",
      "sleep_data['tschq-stress'] = sleep_data['tschq_q24_stress_correlation']\n",
      "sleep_data['tschq-sleep'] = sleep_data['tschq_q23_sleep_correlation']\n",
      "sleep_data['tschq-nap'] = sleep_data['tschq_q22_nap_change']\n",
      "sleep_data['tschq-somatic'] = sleep_data['tschq_q21_head_change']\n",
      "sleep_data['tschq-sounds-worsen-tinnitus'] = sleep_data['tschq_q20_context_change']\n",
      "sleep_data['tschq-sounds-suppress-tinnitus'] = sleep_data['tschq_q19_context_volume']\n",
      "sleep_data['tschq-n-treatments'] = sleep_data['tschq_q18_treatment_count']\n",
      "sleep_data['tschq-tinpitch'] = sleep_data['tschq_q15_tone_frequency']\n",
      "sleep_data['tschq-tintype'] = sleep_data['tschq_q14_tone_type']\n",
      "sleep_data['tschq-fluctuations'] = sleep_data['tschq_q11_daily_volume']\n",
      "sleep_data['tschq-intermitent'] = sleep_data['tschq_q10_history']\n",
      "sleep_data['tschq-tinside'] = sleep_data['tschq_q9_perception']\n",
      "sleep_data['tschq-pulsating'] = sleep_data['tschq_q8_pulsating']\n",
      "# maudite\n",
      "sleep_data['tschq-cause'] = sleep_data['tschq_q7_begin_correlation']\n",
      "sleep_data['tschq-innital-perception'] = sleep_data['tschq_q6_begin_perception']\n",
      "sleep_data['tschq-familiy'] = sleep_data['tschq_q4_family']\n"
     ]
    }
   ],
   "source": [
    "#Guide des attributions de variables\n",
    "st =\"\"\"sleep_data['tschq.personal_loudness = tschq_q12_personal_volume,\n",
    "sleep_data['tschq.months_onset = tschq_months_since_begin_tinnitus,\n",
    "sleep_data['tschq.awareness_time = tschq_q16_awareness,\n",
    "sleep_data['tschq.angerness_time = tschq_q17_angerness,\n",
    "sleep_data['tschq.psychiatry = tschq_q35_psychological_treatment,\n",
    "sleep_data['tschq.neckpain = tschq_q33_nape_problems,\n",
    "sleep_data['tschq.tmj = tschq_q32_jaw_problems,\n",
    "sleep_data['tschq.vertigo = tschq_q31_bogus,\n",
    "sleep_data['tschq.headache = tschq_q30_headache,\n",
    "sleep_data['tschq.noise.pain = tschq_q29_noise_dependent,\n",
    "sleep_data['tschq.hyperacusis = tschq_q28_noise_sensitive,\n",
    "sleep_data['tschq.ha = tschq_q27_hear_helps,\n",
    "sleep_data['tschq.hearproblems = tschq_q26_hear_problems,\n",
    "sleep_data['tschq.stress = tschq_q24_stress_correlation,\n",
    "sleep_data['tschq.sleep = tschq_q23_sleep_correlation,\n",
    "sleep_data['tschq.nap = tschq_q22_nap_change,\n",
    "sleep_data['tschq.somatic = tschq_q21_head_change,\n",
    "sleep_data['tschq.sounds.worsen.tinnitus = tschq_q20_context_change,\n",
    "sleep_data['tschq.sounds.suppress.tinnitus = tschq_q19_context_volume,\n",
    "sleep_data['tschq.n.treatments = tschq_q18_treatment_count,\n",
    "sleep_data['tschq.tinpitch = tschq_q15_tone_frequency,\n",
    "sleep_data['tschq.tintype = tschq_q14_tone_type,\n",
    "sleep_data['tschq.fluctuations = tschq_q11_daily_volume,\n",
    "sleep_data['tschq.intermitent = tschq_q10_history,\n",
    "sleep_data['tschq.tinside = tschq_q9_perception,\n",
    "sleep_data['tschq.pulsating = tschq_q8_pulsating,\n",
    "# maudite\n",
    "sleep_data['tschq.cause = tschq_q7_begin_correlation,\n",
    "sleep_data['tschq.innital.perception = tschq_q6_begin_perception,\n",
    "sleep_data['tschq.familiy = tschq_q4_family,\"\"\"\n",
    "st = st.replace(\".\",\"-\")\n",
    "st = st.replace(\" = \",\"'] = sleep_data['\")\n",
    "st = st.replace(\",\",\"']\")\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data['tschq-personal_loudness'] = sleep_data['tschq_q12_personal_volume']\n",
    "sleep_data['tschq-months_onset'] = sleep_data['tschq_months_since_begin_tinnitus']\n",
    "sleep_data['tschq-awareness_time'] = sleep_data['tschq_q16_awareness']\n",
    "sleep_data['tschq-angerness_time'] = sleep_data['tschq_q17_angerness']\n",
    "sleep_data['tschq-psychiatry'] = sleep_data['tschq_q35_psychological_treatment']\n",
    "sleep_data['tschq-neckpain'] = sleep_data['tschq_q33_nape_problems']\n",
    "sleep_data['tschq-tmj'] = sleep_data['tschq_q32_jaw_problems']\n",
    "sleep_data['tschq-vertigo'] = sleep_data['tschq_q31_bogus']\n",
    "sleep_data['tschq-headache'] = sleep_data['tschq_q30_headache']\n",
    "sleep_data['tschq-noise-pain'] = sleep_data['tschq_q29_noise_dependent']\n",
    "sleep_data['tschq-hyperacusis'] = sleep_data['tschq_q28_noise_sensitive']\n",
    "sleep_data['tschq-ha'] = sleep_data['tschq_q27_hear_helps']\n",
    "sleep_data['tschq-hearproblems'] = sleep_data['tschq_q26_hear_problems']\n",
    "sleep_data['tschq-stress'] = sleep_data['tschq_q24_stress_correlation']\n",
    "sleep_data['tschq-sleep'] = sleep_data['tschq_q23_sleep_correlation']\n",
    "sleep_data['tschq-nap'] = sleep_data['tschq_q22_nap_change']\n",
    "sleep_data['tschq-somatic'] = sleep_data['tschq_q21_head_change']\n",
    "sleep_data['tschq-sounds-worsen-tinnitus'] = sleep_data['tschq_q20_context_change']\n",
    "sleep_data['tschq-sounds-suppress-tinnitus'] = sleep_data['tschq_q19_context_volume']\n",
    "sleep_data['tschq-n-treatments'] = sleep_data['tschq_q18_treatment_count']\n",
    "sleep_data['tschq-tinpitch'] = sleep_data['tschq_q15_tone_frequency']\n",
    "sleep_data['tschq-tintype'] = sleep_data['tschq_q14_tone_type']\n",
    "sleep_data['tschq-fluctuations'] = sleep_data['tschq_q11_daily_volume']\n",
    "sleep_data['tschq-intermitent'] = sleep_data['tschq_q10_history']\n",
    "sleep_data['tschq-tinside'] = sleep_data['tschq_q9_perception']\n",
    "sleep_data['tschq-pulsating'] = sleep_data['tschq_q8_pulsating']\n",
    "# maudite\n",
    "sleep_data['tschq-cause'] = sleep_data['tschq_q7_begin_correlation']\n",
    "sleep_data['tschq-innital-perception'] = sleep_data['tschq_q6_begin_perception']\n",
    "sleep_data['tschq-familiy'] = sleep_data['tschq_q4_family']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 50.4, nan, nan, nan, 75.4167, nan, nan, nan, nan, nan, 47.6, nan, nan, 45.6, nan, nan, nan, nan, nan, nan, nan, 13.6, nan, nan, nan, nan, nan, nan, nan, nan, 42.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 57.9167, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 48.0, 63.6, nan, nan, nan, nan, nan, 72.0, 72.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 78.8, 78.8, nan, nan, nan, 57.2, 38.0, nan, 71.6, 35.6, nan, nan, nan, nan, nan, nan, nan, nan, 43.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 90.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 64.5833, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 60.4, 60.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 27.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 77.0833, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 26.8, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 95.2, nan, nan, nan, nan, nan, 34.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 88.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 40.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 53.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 67.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 51.2, nan, nan, nan, nan, nan, nan, nan, 58.0, nan, 50.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 56.8, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 43.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 75.2, 57.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 68.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 93.6, nan, 83.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 88.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 74.0, 69.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 79.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 94.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 42.0833, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 78.8, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 28.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 49.6, nan, nan, nan, nan, nan, nan, nan, 40.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 35.2, 55.6, nan, nan, nan, nan, 71.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 53.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 47.2, nan, nan, nan, nan, nan, 55.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 49.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 62.9167, nan, nan, nan, 37.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 66.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 78.8, 89.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 91.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 64.1667, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 93.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 73.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 76.8, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 38.75, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 27.2, nan, nan, 83.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 44.3478, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 45.8333, nan, nan, nan, nan, nan, nan, nan, nan, 47.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 21.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, 73.6, nan, nan, nan, nan, nan, nan, 40.4167, nan, nan, nan, nan, nan, nan, nan, 34.0, nan, nan, nan, nan, 23.2, nan, 26.0, nan, nan, nan, nan, 73.6, 72.0, nan, nan, nan, nan, nan, nan, nan, 55.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 54.1667, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 27.7273, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 35.2, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 70.0, 76.4, 70.8, nan, nan, nan, nan, nan, nan, 37.6, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 31.6, nan, nan, nan, nan, nan, nan, 43.6, nan, nan, nan, nan, nan, nan, nan, nan, 63.2, 38.4, nan, nan, 48.0, nan, 47.2, 42.4, 79.2, nan, nan, nan, nan, nan, nan, nan, 68.75, nan, 62.8, nan, nan, nan, 39.1667, 44.0, nan, nan, nan, 59.2, nan, nan, nan, nan, 68.8, nan, nan, 44.3478, 73.913, 87.6, 54.8, nan, nan, 42.0, 60.0, nan, 66.4, 68.4, nan, 42.0, 49.2, 77.2, nan, 48.8, nan, 60.4, nan, nan, nan, nan, 84.8, 58.0, 45.2, nan, 41.3636, nan, 71.2, 51.2, nan, 58.0, 51.6, 36.4, 56.8, nan, 32.0, nan, 71.2, 62.6087, nan, nan, 29.6, 22.4, 18.4, 67.6, nan, 18.8, 40.4167, nan, 80.8333, nan, 60.8, 86.5217, nan, 59.5833, nan, 52.8, 80.0, nan, nan, nan, 77.2, nan, 54.0, 70.8, nan, 65.0, 24.0, 47.9167, 60.4167, 12.8, nan, 28.4, 37.5, 78.0, nan, 41.6, nan, 37.6, nan, 46.8, 81.6, nan, 59.1667, 13.2, nan, 89.5652, 95.6, nan, 37.6, nan, 64.4, 72.0, 50.8333, nan, 57.8261, 38.8, nan, 30.0, nan, 72.0, 41.6, nan, 27.6, 58.0, 61.3043, 34.4, nan, 66.8, nan, nan, nan, 37.6, nan, 43.6, 50.4348, nan, 71.6, nan, nan, 73.2, 54.0, nan, 43.2, 62.4, nan, 29.1304, 80.4, nan, 63.2, nan, 65.2, 43.2, 50.8, nan, 49.2, nan, 57.2, 57.2, 45.2, nan, 33.2, 29.6, 35.6, nan, 45.4167, nan, 19.6, 64.8, 37.2, nan, 81.2, 58.4, 89.6, 63.6, nan, 50.4, nan, 63.6, 50.0, 36.4, nan, 50.0, nan, 43.6, nan, 43.913, 37.6, nan, 69.2, 100.0, nan, 73.2, 33.2, 67.6, 45.2174, nan, 13.6, 88.4, nan, nan, 30.8, nan, 30.4, nan, 96.8, nan, 64.8, 66.0, 52.0, nan, 62.0, 100.0, 56.8182, 15.6, 64.0, 41.2, nan, 21.6, 69.2, nan, 39.6, 74.0, nan, 41.2, 58.0, nan, 14.5833, nan, 30.8, nan, 99.6, 90.0, 68.0, 59.2, 82.8, nan, 30.0, nan, 29.2, 77.2, nan, 26.4, nan, 81.6, nan, 21.6, nan, nan, 55.2, 62.5, nan, 70.4, 23.6, 31.25, nan, 31.2, 72.8, 37.2, nan, 40.4, nan, nan, 50.9091, 51.2, nan, 52.8, 12.4, 67.6, nan, 80.4, 16.8, nan, 58.0, nan, 5.8333, 67.2, 59.2, 58.0, 51.2, 48.0, nan, 59.4737, nan, nan, nan, 52.8, 23.2, 27.2, nan, nan, nan, 47.0833, 59.6, 22.4, 83.2, 60.0, 41.6, nan, nan, 55.2, nan, 41.6, 50.8, 26.0, 69.1667, 56.4, 23.2, 44.8, 52.0, 43.6, 66.4, 76.8, 40.0, 33.75, 26.0, 29.2, 56.0, 60.0, nan, 82.0, 65.6, 85.2, 37.6, 82.0, nan, nan, 61.2, 38.0, 28.0, 41.2, nan, 84.4, 52.8, 16.8, 70.4, 77.0, 51.6, 64.8, 25.2, nan, nan, nan, 54.0, 46.8, 20.0, nan, 18.0, 71.6, nan, 35.2, 56.8, 46.4, 46.4, 46.4, 82.4, nan, 52.4, 47.6, 36.8, 26.4, 23.6, 62.8, 44.0, 66.087, 79.5833, 39.6, nan, 62.4, nan, 95.4545, 90.8, nan, 79.6, 57.2, 66.4, 68.8, 71.2, 80.4, 31.2, 49.1667, 14.8, 46.0, 70.8, 74.8, 10.4, 13.2, 80.0, 41.6, 50.8, 20.8, 50.8333, 33.6, 24.1667, 48.0, 45.2, nan, 82.4, 70.0, 56.8, 78.4, 77.2, 40.8, 44.8, nan, 87.0833, 64.4, 72.4, 41.2, 52.8, 29.2, 51.6, 69.6, 73.6, 58.8, 43.2, 24.0, 47.2, 52.8, 61.2, 62.0, 34.8, 5.6, 77.6, 10.0, nan, nan, nan, 40.8, 14.0, 23.2, 51.6, 60.8, 51.6, 53.6, 43.6, nan, 66.0, 75.2, 82.6087, 71.3636, 15.6, 51.6, 35.2, 39.6, 38.8, 69.6, 75.2632, 71.2, 78.8, 60.8, nan, 22.0, nan, 30.0, 30.0, 85.2, 29.6, 31.6, 16.4, 45.2, 28.8, 60.0, 82.0, 40.8, 2.4, 78.8, 67.6, 45.6, 61.6, 78.0, 50.5, 25.6, 36.4, 42.8, 45.2, 66.4, 50.4, 66.0, 75.2, 27.6, 33.2, 48.0, 49.2, 48.4, 28.0, 35.6, 55.2, 56.0, 60.0, nan, 84.5833, 27.6, 66.8, 51.25, 90.4, 47.6, 31.2, 22.4, 34.8, 0.8, 6.8, 68.4, 41.6, 36.4, 73.6, 63.2, 78.8, 45.6, 58.0, 81.8182, 36.9565, 88.8, 18.6364, 19.2, 26.0, 29.6, 69.2, 19.2, 11.2, 60.8, 82.8, 26.0, 72.8, 60.4, 67.2, 90.4, 54.4, 66.0, 46.0, 73.2, 15.2, 69.2, 68.0, 23.2, 52.9167, nan, 70.0, 63.2, 87.2, 81.6, nan, nan, 51.3043, 83.2, 85.6, 44.8, 42.0, 60.0, 74.4, 25.2, 25.6, 44.8, 64.4, 53.6, 70.0, 84.0, 53.2, 46.4, 61.6, 62.8, 41.6, 40.4, 35.0, 29.2, 24.4, 62.8, 29.2, nan, nan, 20.0, 84.8, 60.8, 37.2, 46.4, 42.4, 71.2, 68.0, 88.0, 50.8, 76.4, 5.6, 51.2, 53.2, 94.4, 57.2, 48.8, 48.8, 27.6, 56.4, 52.4, 24.0, 38.8, 79.6, 54.0, 86.8, 29.2, 62.4, 65.2, 27.2, 44.0, 71.6667, 48.3333, 23.6, 88.0, 66.0, 74.4, 77.2, 97.6, 88.4, 46.8, 26.5217, 90.4545, 91.25, 14.4, 50.0, nan, 62.0, 80.4, 68.8, 94.0, 45.2, 98.4, 42.0833, 19.6, 78.8, 48.0, 98.8, 62.0, 76.8, 55.6, 34.0, 95.6, 89.0, 43.2, 27.6, 65.6, 67.2, 47.2, 60.0, 75.2, 40.4, 96.0, 61.6, 17.2, 81.2, 15.2, 82.8, 93.2, 64.4, 52.9167, 64.8, 50.8, 70.0, 38.0, 68.4, 24.8, 32.8, 60.4167, 49.2, 64.1667, 74.8, 62.4, 87.2, 66.0, 73.913, 46.4, 59.2, 41.2, 35.8333, 100.0, 77.2, 78.8, 60.8, 63.6, 30.0, 41.2, 42.4, 48.4, 20.0, 36.8, 22.8, 68.6957, 76.5217, 36.4, 32.4, 53.6, 63.6, 74.0, 58.0, nan, 25.2, 45.8333, 94.0, 35.6, 56.6667, 13.2, 36.0, 84.4, 28.8, 22.8, 59.2, 53.2, 50.8, nan, 21.6667, 75.2, 77.6, 60.4, 18.4, 29.2, 65.2, 84.8, 56.0, 45.2, 71.6, 97.2, 53.6, 66.4, 41.6, 47.5, 64.8, 43.2, 58.4, 90.4, 95.8333, 4.0909, 59.6, nan, 58.4, 88.0, 98.8, 86.0, 26.8, 60.0, 62.0, 41.6, 77.6, 30.4, 88.5714, 84.0, 42.4, 37.6, 62.0, 82.0, 84.0, 26.4, 38.8, 83.2, 26.0, 74.8, 81.6, 42.8, 64.0, 58.8, 45.0, 25.2, 74.8, 56.4, 24.8, 25.2, 56.8, 35.6, 47.6, 39.6, 37.2, 68.8, 43.6, 64.8, 60.0, 61.6, 52.4, 53.2, 45.6, 49.6, 94.0, 79.5833, 46.0, 90.0, 26.8, 65.6, 44.0, 70.0, 36.8, 53.2, 43.2, 54.4, 36.8, 63.2, 61.2, 32.0, 57.6, 64.4, 61.6, 72.4, 56.8, 31.6, 52.9167, 13.6, 87.6, 60.4, 10.8, 54.0, 23.2, 28.4, 60.0, 57.2, 52.8, 54.0, 76.6667, 37.6, 40.8, 87.6, 75.6, 57.2, 75.6, 75.6, 58.4, 78.0, 45.6, 46.4, 33.6, 54.8, 55.6, 10.8, 30.4, 62.0, 30.8, 22.9167, 45.2, 64.0, 13.6, 40.0, 60.8, 22.0, 74.0, 6.4, 19.2, 30.8, 18.0, 41.6, 59.6, 45.2, 69.6, 40.4, 36.0, 93.2, 67.2, 28.0, 64.0, 56.4, 88.4, 52.8, 70.8333, 50.4, 66.4, 65.2, 62.1739, 16.8, 51.2, 64.0, 37.2, 47.8261, 57.2, 53.6, 12.8, 54.0, 60.4545, 46.0, 50.8, 52.0, 46.4, 88.0, 74.4, nan, 26.4, 38.4, 46.0, 36.0, 67.6, 63.2, 68.0, 37.6, 66.6667, 19.6, 96.8, 39.6, 54.4, 63.6, 39.2, 74.0, 16.0, 69.2, 62.0, 42.8, 44.4, 70.4, 42.0, 38.4, 62.4, 87.6, 61.9048, 33.6, 53.2, 62.0, nan, 33.6, 80.4, 73.6, 22.5, 81.6, 72.0, 75.6, 70.4, 24.8, 20.0, 77.6, 25.6, 36.9565, 64.4, nan, 76.0, 66.8, 41.6, 35.6, 80.8, 42.4, 84.8, 54.8, 39.2, 54.0, 32.8, 68.8, 31.6, 58.0, 82.4, 82.4, 18.0, 14.4, 36.4, 46.0, 66.8, 39.6, 84.4, 40.4, 66.8, 44.0, 38.8, 62.9167, 53.6, 43.2, 97.6, 61.2, 15.6, 34.4, 53.2, 61.2, 66.0, 78.4, 80.4, 20.0, 48.4, 91.2, 50.0, 41.6, 72.8, 58.8, 60.8, 30.8, 68.0, 57.2, 43.75, 86.25, 54.0, 66.4, 58.0, 38.0, 21.6, 28.0, 32.0, 28.0, 75.6, 76.8, 37.5, 71.6, 31.2, 62.4, 79.2, 38.4, 13.2, 63.6, 62.0, 33.6, 47.6, 55.6, 50.0, 44.4, 81.6, 55.6, 41.2, 53.2, 28.8, 16.8, 74.4, 13.75, 37.6, 29.2, 58.4, 30.4, 12.4, 16.4, 44.8, 54.8, 88.8, 33.2, 46.4, 60.0, 86.0, 69.2, 58.0, 31.2, 39.2, 32.8, 26.8, 52.4, 57.2, 72.8, 59.6, 57.2, 40.0, 55.6, 65.6, 60.0, 52.8, 46.0, 48.4, 77.6, 84.0, 75.6, 46.0, 97.6, 83.2, 25.6, 53.6, 71.6, 74.4, 54.8, 51.6, 33.5, 61.6, 56.0, 56.25, 45.2, 55.6, 52.4, 40.8, 33.2, 80.8, 76.6667, 24.0, 47.2, 52.0, 77.2, 27.6, 26.8, 54.4, 81.2, 70.0, 85.6, 54.8, 63.6, 79.2, 47.6, 25.6, 63.2, 76.4, 74.4, 57.6, 60.0, 34.0, 43.6, 71.2, 57.2, 38.4, 92.8, 55.6, 57.6, 78.8, 48.8, 80.0, 22.8, 43.2, 10.4, 60.0, 52.0, 24.0, 80.0, 57.6, 60.0, 83.75, 47.6, 23.6, 18.8, 35.6, 31.6, 44.4, 53.5, 83.2, 88.8, 68.4, 27.2, 28.0, 16.4, 74.7826, 69.6, nan, 22.0, 78.8, nan, 83.2, 65.2, 60.8, 60.8, 74.4, nan, 51.2, 30.8, 47.2, 60.8, 91.6, 74.0, 63.2, 48.8, 35.0, 28.2609, 57.6, 17.2, 34.8, 37.2, 71.6, 58.4, 24.8, 12.4, 92.4, 61.2, 28.8, 62.4, 74.0, 66.4, 31.6, 23.6, 75.2, 72.8, 43.6, 31.6, 87.5, 51.2, 24.4, 51.6, 23.6, 19.2, 78.8, 27.6, 83.0, 70.8, 47.6, 94.0, 5.2, 84.8, nan, 36.8, 55.2, 68.3333, 38.4, 56.25, 54.3478, 79.6, 63.6, 57.6, 50.4, nan, 1.6, 47.2, 37.2, 75.6, 49.6, 28.4, 78.0, 71.2, 79.6, 37.6, 41.2, 68.8, 68.4, 47.6, 58.0, 25.2, 27.6, 60.0, 82.8, 57.0833, 34.4, 33.2, 62.0, 86.4, 59.6, 82.8, 35.2, 34.0, 56.0, 81.2, 98.4, 26.8, 74.8, 26.8, 40.8, 37.6, 7.2, 20.0, 61.2, 66.4, 32.8, 62.0, 54.8, 17.6, 50.8, 91.6, 53.2, 36.8, 87.6, 65.6, 40.8, 32.4, 58.0, 64.0, 50.8, 91.2, 67.8261, 43.3333, 72.4, 58.8, 62.4, 72.4, 64.0, 57.2, 59.2, 39.2, 82.8, 40.4, 49.2, 80.4, 52.6087, 18.4, 35.6, 34.4, 97.2, 42.4, 55.6, 20.4, 11.2, 44.8, 42.8, 72.5, 56.8, 65.6, 70.0, 29.2, 50.8, 70.4, 35.6, 27.6, 14.4, 71.6, 98.4, 10.8, 54.0, 59.2, 48.4, 88.0, 34.4, 80.0, 67.6, 53.6, 23.6, 6.4, 88.4, 85.9091, 66.0, 46.0, 6.8, 98.0, 58.8, 36.8, 29.6, 30.8, 66.4, 30.4, 16.0, 32.8, 42.8, 57.2, 63.6, 96.0, 65.2, 9.6, 28.0, 32.8, 32.4, nan, 86.8, 84.0, 65.2, 38.4, 24.8, 36.0, 68.0, 40.0, 89.2, 48.4, 83.3333, 35.6, 59.6, 57.2, 57.2, 73.2, 43.6, 52.8, 36.0, 75.2, 37.2, 91.6, 19.1667, 53.2, 64.4, 23.6, 46.8, 53.6364, 52.4, 18.4, 74.8, 65.2, 61.2, 48.0, 38.8, 59.6, 55.6, 22.4, 41.6, 32.0, 45.6, 69.6, 22.0, 90.0, 37.2, 95.6, 16.4, 64.8, 74.3478, 40.4, 33.2, 62.4, 78.8, 84.8, 51.2, 49.6, 54.0, 65.6, 43.4783, 38.8, 21.2, 8.0, 27.2, 75.6, 47.2, 71.6, 73.2, 65.6, 67.2, 79.6, 37.6, 54.4, 59.2, 86.4, 31.6, 30.0, 85.6, 66.0, 68.75, 76.4, 62.0, 74.8, 51.2, 48.4, 47.6, 84.4, 50.4167, 78.4, 84.4, 34.8, 50.0, 2.4, 38.0, 62.0, 41.2, 89.2, 12.8, 53.6, 8.0, 27.6, 51.2, 41.2]\n"
     ]
    }
   ],
   "source": [
    "old_tfi = sleep_data['tfi_score']\n",
    "print(list(old_tfi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verifier si ça fait du bon boulot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
      "<ipython-input-155-7080acb24337>:13: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "tfi_scores_recipes = pd.DataFrame({\n",
    "    'score': ['tfi_score', 'tfi_intrusive', 'tfi_sense_of_control', 'tfi_cognitive', 'tfi_sleep', 'tfi_auditory', 'tfi_relaxation', 'tfi_qol', 'tfi_emotional'],\n",
    "    'regex': ['^tfi_[0-9]{1,2}$', '^tfi_[123]$', '^tfi_[456]$', '^tfi_[789]$', '^tfi_1[012]$', '^tfi_1[345]$', '^tfi_1[678]$', '^tfi_(19|20|21|22)$', '^tfi_2[345]$'],\n",
    "    'n_max_missing_items': [6, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "})\n",
    "\n",
    "def compute_tfi_scores(x, score, regex, n_max_missing_items):\n",
    "    x_subset = x.filter(regex=regex)\n",
    "    res = x_subset.mean(axis=1) * 10\n",
    "    res[x_subset.isna().sum(axis=1) > n_max_missing_items] = pd.np.nan\n",
    "    return pd.DataFrame({score: res})\n",
    "\n",
    "longitudinal_data_tfi = sleep_data.filter(regex='^tfi_')\n",
    "longitudinal_data_tfi[['tfi_1','tfi_3']] = longitudinal_data_tfi[['tfi_1','tfi_3']] // 10\n",
    "tfi_scores = longitudinal_data_tfi.pipe(lambda x: pd.concat([compute_tfi_scores(x, *args) for args in tfi_scores_recipes.values], axis=1))\n",
    "sleep_data = pd.concat([sleep_data.drop(columns='tfi_score'), tfi_scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 50.4, nan, nan, nan, 75.4167, nan, nan, nan, nan, nan, 47.6, nan, nan, 45.6, nan, nan, nan, nan, nan, nan, nan, 13.6, nan, nan, nan, nan, nan, nan, nan, nan, 42.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 57.9167, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 48.0, 63.6, nan, nan, nan, nan, nan, 72.0, 72.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 78.8, 78.8, nan, nan, nan, 57.2, 38.0, nan, 71.6, 35.6, nan]\n",
      "[nan, 50.4, nan, nan, nan, 75.41666666666667, nan, nan, nan, nan, nan, 47.599999999999994, nan, nan, 45.599999999999994, nan, nan, nan, nan, nan, nan, nan, 13.600000000000001, nan, nan, nan, nan, nan, nan, nan, nan, 42.400000000000006, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 57.91666666666667, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 48.0, 63.6, nan, nan, nan, nan, nan, 72.0, 72.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 78.8, 78.8, nan, nan, nan, 57.199999999999996, 38.0, nan, 71.6, 35.6, nan]\n"
     ]
    }
   ],
   "source": [
    "#print(list(sleep_data.columns))\n",
    "print(list(old_tfi)[0:100])\n",
    "print(list(sleep_data['tfi_score'])[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['center_name', 'external_id', 'id', 'constant_id', 'age', 'sex',\n",
      "       'session_num', 'treatment_code', 'visit_type', 'position',\n",
      "       ...\n",
      "       'tschq-familiy', 'tfi_score', 'tfi_intrusive', 'tfi_sense_of_control',\n",
      "       'tfi_cognitive', 'tfi_sleep', 'tfi_auditory', 'tfi_relaxation',\n",
      "       'tfi_qol', 'tfi_emotional'],\n",
      "      dtype='object', length=248)\n",
      "248\n"
     ]
    }
   ],
   "source": [
    "print(sleep_data.columns)\n",
    "print(len(list(sleep_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = sleep_data.columns[sleep_data.columns.str.startswith('tschq_') |\n",
    "                                     sleep_data.columns.str.contains('validation') |\n",
    "                                     sleep_data.columns.str.contains('whoqol_bref_') |\n",
    "                                     sleep_data.columns.str.contains('bfi') |\n",
    "                                     sleep_data.columns.str.contains('visit_type') |\n",
    "                                     sleep_data.columns.str.contains('center_name') |\n",
    "                                     sleep_data.columns.str.contains('treatment_code') |\n",
    "                                     sleep_data.columns.str.contains('session_num') |\n",
    "                                     sleep_data.columns.str.contains('visit_day') |\n",
    "                                     sleep_data.columns.str.contains('position') |\n",
    "                                     sleep_data.columns.str.contains('id') |\n",
    "                                     sleep_data.columns.str.contains('constant_id')]\n",
    "sleep_data = sleep_data[sleep_data[\"position\"]==1].drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'mdi_1', 'mdi_2', 'mdi_3', 'mdi_4', 'mdi_5', 'mdi_6',\n",
      "       'mdi_7', 'mdi_8a',\n",
      "       ...\n",
      "       'tschq-familiy', 'tfi_score', 'tfi_intrusive', 'tfi_sense_of_control',\n",
      "       'tfi_cognitive', 'tfi_sleep', 'tfi_auditory', 'tfi_relaxation',\n",
      "       'tfi_qol', 'tfi_emotional'],\n",
      "      dtype='object', length=166)\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "print(sleep_data.columns)\n",
    "print(len(list(sleep_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data.to_csv(\"Jorge_data_clean.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# début des analyses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data[\"contrast_1_naps\"] = sleep_data[\"tschq-nap\"]\n",
    "sleep_data[\"contrast_2_sleep\"] = sleep_data[\"tschq-sleep\"].apply(lambda x: \"yes\" if x == \"yes\" else \"no\" if x == \"no\" else None)\n",
    "sleep_data[\"contrast_2_sleep\"] = sleep_data[\"contrast_2_sleep\"].astype(\"category\")\n",
    "\n",
    "contrast1 = sleep_data[['age', 'sex', *[col for col in sleep_data.columns if col.endswith('score')], 'tfi_sleep', *[col for col in sleep_data.columns if col.startswith('ts_')], *[col for col in sleep_data.columns if col.startswith('whoqol_')], *[col for col in sleep_data.columns if col.startswith('tschq')], *[col for col in sleep_data.columns if col.startswith('contrast_1')]]]\n",
    "contrast1 = contrast1.drop(columns=['tschq-cause', 'tschq-n-treatments'])\n",
    "\n",
    "contrast2 = sleep_data[['age', 'sex', *[col for col in sleep_data.columns if col.endswith('score')], 'tfi_sleep', *[col for col in sleep_data.columns if col.startswith('ts_')], *[col for col in sleep_data.columns if col.startswith('whoqol_')], *[col for col in sleep_data.columns if col.startswith('tschq')], *[col for col in sleep_data.columns if col.startswith('contrast_2')]]]\n",
    "contrast2 = contrast2.drop(columns=['tschq-cause', 'tschq-n-treatments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
